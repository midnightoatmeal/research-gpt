{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO6iteQoQPEMMjsQqIotTe7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/midnightoatmeal/research-gpt/blob/main/research-gpt_phase1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLaJHh4fkuiU",
        "outputId": "4d7c1449-98cd-4c07-9aad-7cb26c0d7455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Feb  8 18:27:23 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nGPU Available: True\n",
            "GPU Model: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "# checking GPU availability and specifications\n",
        "\n",
        "!nvidia-smi\n",
        "import torch\n",
        "print(f\"nGPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "!pip install -q transformers==4.36.2  # Latest stable version with good Mistral support\n",
        "!pip install -q datasets==2.16.1      # For handling our research paper dataset\n",
        "!pip install -q peft==0.7.1           # For parameter-efficient fine-tuning\n",
        "!pip install -q accelerate==0.25.0    # For optimized training\n",
        "!pip install -q arxiv==2.0.0          # For fetching papers from arXiv\n",
        "!pip install -q evaluate==0.4.1       # For model evaluation\n",
        "!pip install -q wandb==0.16.2         # For experiment tracking\n",
        "!pip install -q rouge-score==0.1.2    # For evaluating text generation\n",
        "\n",
        "!pip list | grep -E \"transformers|datasets|peft|accelerate|arxiv|evaluate|wandb|rouge-score\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa4GrDebm0AT",
        "outputId": "29614aa8-39bb-48eb-8c92-95dfe2d1f5c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "PyTorch version: 2.5.1+cu124\n",
            "CUDA available: True\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.10.0 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.10.0 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "accelerate                         0.25.0\n",
            "arxiv                              2.0.0\n",
            "datasets                           2.16.1\n",
            "evaluate                           0.4.1\n",
            "peft                               0.7.1\n",
            "sentence-transformers              3.4.1\n",
            "tensorflow-datasets                4.9.7\n",
            "transformers                       4.36.2\n",
            "vega-datasets                      0.9.0\n",
            "wandb                              0.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the project structure\n",
        "\n",
        "import os\n",
        "\n",
        "def create_directory_structure():\n",
        "  \"\"\"Creates a structured directory system for my research project.\"\"\"\n",
        "  directories = [\n",
        "      'data/raw',\n",
        "      'data/processed',\n",
        "      'models/checkpoints',\n",
        "      'experiments/logs',\n",
        "      'experiments/results',\n",
        "      'src'\n",
        "  ]\n",
        "\n",
        "  for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    print(f\"Created directory: {directory}\")\n",
        "\n",
        "create_directory_structure()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hWsEcQqn41p",
        "outputId": "bf525e61-0888-42f4-ba6e-70ee392612e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: data/raw\n",
            "Created directory: data/processed\n",
            "Created directory: models/checkpoints\n",
            "Created directory: experiments/logs\n",
            "Created directory: experiments/results\n",
            "Created directory: src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wandb\n",
        "\n",
        "# Set your API key (choose one method)\n",
        "os.environ[\"WANDB_API_KEY\"] = \"3b19bd5a40bd69e53ac39df2cc708f51e07394f5\"  # Method 1\n",
        "\n",
        "\n",
        "# Initialize a new wandb run\n",
        "# Think of a 'run' as one complete training session of your model\n",
        "wandb.init(\n",
        "    # Project name - this will group all your runs under this project\n",
        "    project=\"researchgpt\",\n",
        "\n",
        "    # Configuration dictionary - these are the hyperparameters and settings\n",
        "    # you want to track for this training run\n",
        "    config={\n",
        "        \"model_name\": \"mistralai/Mistral-7B-v0.1\",  # The model you're using\n",
        "        \"learning_rate\": 2e-4,                       # Learning rate for training\n",
        "        \"batch_size\": 4,                             # Number of samples processed at once\n",
        "        \"max_length\": 512                            # Maximum sequence length\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "7uekNIGA7TK3",
        "outputId": "d80df894-2e90-456e-fb41-d14c01479476"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlionelrozario98\u001b[0m (\u001b[33mlionelrozario98-personal-project\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.19.6 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250208_182942-46zujlph</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lionelrozario98-personal-project/researchgpt/runs/46zujlph' target=\"_blank\">expert-spaceship-4</a></strong> to <a href='https://wandb.ai/lionelrozario98-personal-project/researchgpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lionelrozario98-personal-project/researchgpt' target=\"_blank\">https://wandb.ai/lionelrozario98-personal-project/researchgpt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lionelrozario98-personal-project/researchgpt/runs/46zujlph' target=\"_blank\">https://wandb.ai/lionelrozario98-personal-project/researchgpt/runs/46zujlph</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lionelrozario98-personal-project/researchgpt/runs/46zujlph?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fa2344e99d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_environment():\n",
        "    \"\"\"\n",
        "    Verifies that all components of our research environment are working correctly.\n",
        "    \"\"\"\n",
        "    checks = {\n",
        "        \"GPU Available\": torch.cuda.is_available(),\n",
        "        \"Project Structure\": os.path.exists(\"data/raw\"),\n",
        "        \"Git Repository\": os.path.exists(\".git\"),\n",
        "        \"Python Version\": sys.version\n",
        "    }\n",
        "\n",
        "    for check, status in checks.items():\n",
        "        print(f\"{check}: {'✅' if status else '❌'}\")\n",
        "\n",
        "test_environment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUdk7UgACzX0",
        "outputId": "5c839d4e-dd96-4e89-b830-08647c7d0386"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: ✅\n",
            "Project Structure: ✅\n",
            "Git Repository: ❌\n",
            "Python Version: ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import List, Dict, Optional, Union\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "class ResearchPaperPipeline:\n",
        "    def __init__(self,\n",
        "                 model_name: str = \"facebook/opt-125m\",\n",
        "                 cache_dir: str = \"data\",\n",
        "                 max_length: int = 512):\n",
        "        \"\"\"\n",
        "        Initialize the research pipeline with configuration parameters.\n",
        "        We start with a smaller model for development, then scale up to Mistral.\n",
        "        \"\"\"\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.max_length = max_length\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Set up logging with timestamp\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        log_file = self.cache_dir / f'pipeline_{timestamp}.log'\n",
        "\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[\n",
        "                logging.FileHandler(log_file),\n",
        "                logging.StreamHandler()\n",
        "            ]\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        # Initialize tokenizer with error handling\n",
        "        try:\n",
        "            self.logger.info(f\"Loading tokenizer for {model_name}\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.logger.info(\"Tokenizer loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading tokenizer: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        # Create necessary directories\n",
        "        for subdir in ['raw', 'processed', 'logs', 'models']:\n",
        "            (self.cache_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.logger.info(\"Pipeline initialized successfully\")\n",
        "\n",
        "    def fetch_papers(self,\n",
        "                    categories: List[str] = ['cs.AI', 'cs.LG'],\n",
        "                    max_results: int = 100,\n",
        "                    save_raw: bool = True,\n",
        "                    start_date: str = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Fetch papers from arXiv with improved error handling and date filtering.\n",
        "\n",
        "        Args:\n",
        "            categories: List of arXiv categories to fetch\n",
        "            max_results: Maximum number of papers to fetch\n",
        "            save_raw: Whether to save raw data\n",
        "            start_date: Optional start date in YYYY-MM-DD format\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Fetching {max_results} papers from categories: {categories}\")\n",
        "\n",
        "        papers = []\n",
        "        search_query = ' OR '.join(f'cat:{cat}' for cat in categories)\n",
        "\n",
        "        # Add date filter if specified\n",
        "        if start_date:\n",
        "            # Convert date to arXiv format (YYYYMMDDHHMMSS)\n",
        "            arxiv_date = f\"{start_date.replace('-', '')}000000\"\n",
        "            search_query += f' AND submittedDate:[{arxiv_date} TO 999999999999]'\n",
        "\n",
        "        client = arxiv.Client(page_size=100, delay_seconds=3.0)\n",
        "        search = arxiv.Search(\n",
        "            query=search_query,\n",
        "            max_results=max_results,\n",
        "            sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            with tqdm(total=max_results, desc=\"Fetching papers\") as pbar:\n",
        "                for paper in client.results(search):\n",
        "                    try:\n",
        "                        papers.append({\n",
        "                            'id': paper.entry_id,\n",
        "                            'title': paper.title,\n",
        "                            'abstract': paper.summary,\n",
        "                            'authors': [author.name for author in paper.authors],\n",
        "                            'categories': paper.categories,\n",
        "                            'published': paper.published,\n",
        "                            'updated': paper.updated,\n",
        "                            'doi': paper.doi,\n",
        "                            'primary_category': paper.primary_category\n",
        "                        })\n",
        "                        pbar.update(1)\n",
        "\n",
        "                        if len(papers) >= max_results:\n",
        "                            break\n",
        "\n",
        "                    except Exception as e:\n",
        "                        self.logger.warning(f\"Error processing paper {paper.entry_id}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in paper fetch process: {str(e)}\")\n",
        "            if papers:  # Save what we have if there's an error\n",
        "                self.logger.info(\"Saving partial results...\")\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        df = pd.DataFrame(papers)\n",
        "\n",
        "        # Save raw data if requested\n",
        "        if save_raw and not df.empty:\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            save_path = self.cache_dir / 'raw' / f'papers_{timestamp}.parquet'\n",
        "            df.to_parquet(save_path)\n",
        "            self.logger.info(f\"Saved raw data to {save_path}\")\n",
        "\n",
        "        self.logger.info(f\"Successfully fetched {len(df)} papers\")\n",
        "        return df\n",
        "\n",
        "    def prepare_training_data(self,\n",
        "                            df: pd.DataFrame,\n",
        "                            template_style: str = 'comprehensive') -> Dict:\n",
        "        \"\"\"\n",
        "        Prepare paper data for model training with enhanced error checking and validation.\n",
        "        Returns both the dataset and preparation metrics.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Preparing training data\")\n",
        "\n",
        "        templates = {\n",
        "            'comprehensive': [\n",
        "                \"Title: {title}\\nAbstract: {abstract}\\n\\nSummarize the key findings of this research paper.\",\n",
        "                \"Based on this abstract, what are the main contributions of the paper titled '{title}'?\",\n",
        "                \"Analyze the research methodology described in: {title}\\nAbstract: {abstract}\"\n",
        "            ],\n",
        "            'simple': [\n",
        "                \"Summarize this paper: {title}\\n{abstract}\",\n",
        "                \"What is the main point of: {title}?\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        selected_templates = templates.get(template_style, templates['simple'])\n",
        "\n",
        "        training_data = []\n",
        "        skipped_papers = 0\n",
        "\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing papers\"):\n",
        "            try:\n",
        "                for template in selected_templates:\n",
        "                    # Validate input data\n",
        "                    if not isinstance(row['title'], str) or not isinstance(row['abstract'], str):\n",
        "                        skipped_papers += 1\n",
        "                        continue\n",
        "\n",
        "                    prompt = template.format(\n",
        "                        title=row['title'].strip(),\n",
        "                        abstract=row['abstract'].strip()\n",
        "                    )\n",
        "\n",
        "                    # Create completion with citation\n",
        "                    completion = f\"Based on the paper (arXiv:{row['id']}), {row['abstract']}\"\n",
        "\n",
        "                    # Validate token length\n",
        "                    total_tokens = len(self.tokenizer.encode(prompt + completion))\n",
        "                    if total_tokens > self.max_length:\n",
        "                        continue\n",
        "\n",
        "                    training_data.append({\n",
        "                        'prompt': prompt,\n",
        "                        'completion': completion,\n",
        "                        'paper_id': row['id'],\n",
        "                        'token_count': total_tokens\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Error processing paper {row.get('id', 'unknown')}: {str(e)}\")\n",
        "                skipped_papers += 1\n",
        "                continue\n",
        "\n",
        "        # Convert to DataFrame for easier analysis\n",
        "        train_df = pd.DataFrame(training_data)\n",
        "\n",
        "        # Save processed data\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        save_path = self.cache_dir / 'processed' / f'training_data_{timestamp}.parquet'\n",
        "        train_df.to_parquet(save_path)\n",
        "\n",
        "        # Prepare metrics\n",
        "        metrics = {\n",
        "            'total_examples': len(train_df),\n",
        "            'unique_papers': len(train_df['paper_id'].unique()),\n",
        "            'skipped_papers': skipped_papers,\n",
        "            'avg_token_count': train_df['token_count'].mean(),\n",
        "            'max_token_count': train_df['token_count'].max(),\n",
        "            'preparation_timestamp': timestamp\n",
        "        }\n",
        "\n",
        "        self.logger.info(f\"Created {len(train_df)} training examples from {metrics['unique_papers']} papers\")\n",
        "        return {'data': train_df, 'metrics': metrics}\n"
      ],
      "metadata": {
        "id": "x_J_MGAPD8ky"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Expanded Data Collection\n",
        "categories = [\n",
        "    'cs.AI',     # Artificial Intelligence\n",
        "    'cs.LG',     # Machine Learning\n",
        "    'cs.CL',     # Computation and Language\n",
        "    'stat.ML'    # Statistics - Machine Learning\n",
        "]\n",
        "\n",
        "# Fetch papers\n",
        "papers_df = pipeline.fetch_papers(\n",
        "    categories=categories,\n",
        "    max_results=100,\n",
        "    save_raw=True\n",
        ")\n",
        "\n",
        "print(f\"Fetched {len(papers_df)} papers\")\n",
        "print(\"\\nDistribution across categories:\")\n",
        "print(papers_df['primary_category'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bexeXJrwGElX",
        "outputId": "b70a47d8-809c-4991-cd8b-463139329e19"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers: 100%|██████████| 100/100 [00:02<00:00, 45.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 100 papers\n",
            "\n",
            "Distribution across categories:\n",
            "primary_category\n",
            "cs.LG              44\n",
            "cs.CL              23\n",
            "cs.CV               8\n",
            "cs.AI               4\n",
            "stat.ML             4\n",
            "cs.RO               2\n",
            "quant-ph            2\n",
            "cs.SD               2\n",
            "cs.SE               2\n",
            "cs.HC               2\n",
            "physics.chem-ph     1\n",
            "math.NA             1\n",
            "cs.CR               1\n",
            "math.ST             1\n",
            "stat.AP             1\n",
            "eess.AS             1\n",
            "eess.IV             1\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Quality Analysis\n",
        "def analyze_paper_quality(df):\n",
        "    analysis = {\n",
        "        \"total_papers\": len(df),\n",
        "        \"avg_abstract_length\": df['abstract'].str.len().mean(),\n",
        "        \"papers_per_category\": df['primary_category'].value_counts().to_dict(),\n",
        "        \"time_span\": {\n",
        "            \"earliest\": df['published'].min(),\n",
        "            \"latest\": df['published'].max()\n",
        "        }\n",
        "    }\n",
        "    return analysis\n",
        "\n",
        "paper_analysis = analyze_paper_quality(papers_df)\n",
        "print(\"\\nPaper Collection Analysis:\")\n",
        "print(json.dumps(paper_analysis, indent=2, default=str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0gFzLAeGCXb",
        "outputId": "4b6b572e-0f10-4583-a5c6-b61e86fb70a6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Paper Collection Analysis:\n",
            "{\n",
            "  \"total_papers\": 100,\n",
            "  \"avg_abstract_length\": 1217.15,\n",
            "  \"papers_per_category\": {\n",
            "    \"cs.LG\": 44,\n",
            "    \"cs.CL\": 23,\n",
            "    \"cs.CV\": 8,\n",
            "    \"cs.AI\": 4,\n",
            "    \"stat.ML\": 4,\n",
            "    \"cs.RO\": 2,\n",
            "    \"quant-ph\": 2,\n",
            "    \"cs.SD\": 2,\n",
            "    \"cs.SE\": 2,\n",
            "    \"cs.HC\": 2,\n",
            "    \"physics.chem-ph\": 1,\n",
            "    \"math.NA\": 1,\n",
            "    \"cs.CR\": 1,\n",
            "    \"math.ST\": 1,\n",
            "    \"stat.AP\": 1,\n",
            "    \"eess.AS\": 1,\n",
            "    \"eess.IV\": 1\n",
            "  },\n",
            "  \"time_span\": {\n",
            "    \"earliest\": \"2025-02-06 12:24:30+00:00\",\n",
            "    \"latest\": \"2025-02-06 18:59:55+00:00\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipeline.prepare_training_data(\n",
        "    papers_df,\n",
        "    template_style='comprehensive'\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Data Metrics: \")\n",
        "print(json.dumps(result['metrics'], indent=2))\n",
        "\n",
        "print(\"\\nSample Training Examples (3 random examples):\")\n",
        "sample_data = result['data'].sample(3)\n",
        "for _, row in sample_data.iterrows():\n",
        "  print(\"\\n--------------------\")\n",
        "  print(\"PROMPT:\")\n",
        "  print(row['prompt'])\n",
        "  print(\"\\nCOMPLETION (first 200 chars):\")\n",
        "  print(row['completion'][:200] + \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcwy5rBz18iW",
        "outputId": "a5275184-0691-42cf-f842-21218d4b18dd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing papers: 100%|██████████| 100/100 [00:00<00:00, 274.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Data Metrics: \n",
            "{\n",
            "  \"total_examples\": 171,\n",
            "  \"unique_papers\": 99,\n",
            "  \"skipped_papers\": 0,\n",
            "  \"avg_token_count\": 357.906432748538,\n",
            "  \"max_token_count\": 509,\n",
            "  \"preparation_timestamp\": \"20250208_185647\"\n",
            "}\n",
            "\n",
            "Sample Training Examples (3 random examples):\n",
            "\n",
            "--------------------\n",
            "PROMPT:\n",
            "Based on this abstract, what are the main contributions of the paper titled 'Realistic Image-to-Image Machine Unlearning via Decoupling and Knowledge Retention'?\n",
            "\n",
            "COMPLETION (first 200 chars):\n",
            "Based on the paper (arXiv:http://arxiv.org/abs/2502.04260v1), Machine Unlearning allows participants to remove their data from a trained\n",
            "machine learning model in order to preserve their privacy, and ...\n",
            "\n",
            "--------------------\n",
            "PROMPT:\n",
            "Based on this abstract, what are the main contributions of the paper titled 'Evaluating Inter-Column Logical Relationships in Synthetic Tabular Data Generation'?\n",
            "\n",
            "COMPLETION (first 200 chars):\n",
            "Based on the paper (arXiv:http://arxiv.org/abs/2502.04055v1), Current evaluations of synthetic tabular data mainly focus on how well joint\n",
            "distributions are modeled, often overlooking the assessment o...\n",
            "\n",
            "--------------------\n",
            "PROMPT:\n",
            "Based on this abstract, what are the main contributions of the paper titled 'VTutor: An Open-Source SDK for Generative AI-Powered Animated Pedagogical Agents with Multi-Media Output'?\n",
            "\n",
            "COMPLETION (first 200 chars):\n",
            "Based on the paper (arXiv:http://arxiv.org/abs/2502.04103v1), The rapid evolution of large language models (LLMs) has transformed\n",
            "human-computer interaction (HCI), but the interaction with LLMs is cur...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_paper_quality(df):\n",
        "  analysis = {\n",
        "      \"total_papers\": len(df),\n",
        "      \"avg_abstract_lenght\": df['abstract'].str.len().mean(),\n",
        "      \"papers_per_category\": df['primary_category'].value_counts().to_dict(),\n",
        "      \"time_span\":{\n",
        "          \"earliest\": df['published'].min(),\n",
        "          \"latest\": df['published'].max()\n",
        "      },\n",
        "      # adding more detailed analysis\n",
        "      \"authors_per_paper\": df['authors'].apply(len).mean(),\n",
        "      \"papers_with_doi\": df['doi'].notna().sum()\n",
        "\n",
        "    }\n",
        "  return analysis\n",
        "\n",
        "paper_analysis = analyze_paper_quality(papers_df)\n",
        "print(\"\\nPaper Collection Analysis: \")\n",
        "print(json.dumps(paper_analysis, indent=2, default=str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_uPHN5E3EWL",
        "outputId": "02860a24-c599-4d05-a7ce-21f7832f0a53"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Paper Collection Analysis: \n",
            "{\n",
            "  \"total_papers\": 100,\n",
            "  \"avg_abstract_lenght\": 1217.15,\n",
            "  \"papers_per_category\": {\n",
            "    \"cs.LG\": 44,\n",
            "    \"cs.CL\": 23,\n",
            "    \"cs.CV\": 8,\n",
            "    \"cs.AI\": 4,\n",
            "    \"stat.ML\": 4,\n",
            "    \"cs.RO\": 2,\n",
            "    \"quant-ph\": 2,\n",
            "    \"cs.SD\": 2,\n",
            "    \"cs.SE\": 2,\n",
            "    \"cs.HC\": 2,\n",
            "    \"physics.chem-ph\": 1,\n",
            "    \"math.NA\": 1,\n",
            "    \"cs.CR\": 1,\n",
            "    \"math.ST\": 1,\n",
            "    \"stat.AP\": 1,\n",
            "    \"eess.AS\": 1,\n",
            "    \"eess.IV\": 1\n",
            "  },\n",
            "  \"time_span\": {\n",
            "    \"earliest\": \"2025-02-06 12:24:30+00:00\",\n",
            "    \"latest\": \"2025-02-06 18:59:55+00:00\"\n",
            "  },\n",
            "  \"authors_per_paper\": 4.9,\n",
            "  \"papers_with_doi\": \"2\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the updated pipeline\n",
        "pipeline = ResearchPaperPipeline()\n",
        "\n",
        "# Define categories\n",
        "categories = [\n",
        "    'cs.AI',     # Core AI\n",
        "    'cs.LG',     # Machine Learning\n",
        "    'cs.CL',     # NLP\n",
        "    'stat.ML',   # Statistical ML\n",
        "    'cs.CV'      # Computer Vision\n",
        "]\n",
        "\n",
        "# Fetch papers with date filter\n",
        "papers_df = pipeline.fetch_papers(\n",
        "    categories=categories,\n",
        "    max_results=200,\n",
        "    start_date=\"2024-01-01\"\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nFetched {len(papers_df)} papers\")\n",
        "print(\"\\nDistribution across categories:\")\n",
        "print(papers_df['primary_category'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgbsJcML4ekl",
        "outputId": "e8a3f381-600d-4f42-ea16-da7a1fb5b01d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Fetching papers: 100%|██████████| 200/200 [00:06<00:00, 29.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetched 200 papers\n",
            "\n",
            "Distribution across categories:\n",
            "primary_category\n",
            "cs.CV             153\n",
            "eess.IV            15\n",
            "cs.LG              12\n",
            "cs.RO               9\n",
            "cs.MM               2\n",
            "cs.CL               2\n",
            "cs.CR               1\n",
            "cs.CY               1\n",
            "cs.HC               1\n",
            "cs.GR               1\n",
            "cs.AI               1\n",
            "physics.med-ph      1\n",
            "hep-ex              1\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_balanced_papers_v2(pipeline, categories_dict, papers_per_category=40):\n",
        "    \"\"\"\n",
        "    Enhanced version with stricter category filtering and error handling\n",
        "    \"\"\"\n",
        "    all_papers = []\n",
        "\n",
        "    for category in categories_dict:\n",
        "        print(f\"\\nFetching papers for {category}...\")\n",
        "        # Add primary category filter to query\n",
        "        category_papers = pipeline.fetch_papers(\n",
        "            categories=[category],\n",
        "            max_results=papers_per_category * 2,  # Fetch more to filter\n",
        "            start_date=\"2024-01-01\"\n",
        "        )\n",
        "\n",
        "        # Filter to keep only papers where this is the primary category\n",
        "        category_papers = category_papers[\n",
        "            category_papers['primary_category'] == category\n",
        "        ].head(papers_per_category)\n",
        "\n",
        "        all_papers.append(category_papers)\n",
        "\n",
        "    combined_df = pd.concat(all_papers, ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "# Define core categories with strict filtering\n",
        "core_categories = [\n",
        "    'cs.AI',    # Artificial Intelligence\n",
        "    'cs.LG',    # Machine Learning\n",
        "    'cs.CL',    # Computational Linguistics\n",
        "    'stat.ML',  # Statistical Machine Learning\n",
        "    'cs.CV'     # Computer Vision\n",
        "]\n",
        "\n",
        "# Fetch balanced dataset\n",
        "balanced_papers_v2 = fetch_balanced_papers_v2(pipeline, core_categories, papers_per_category=40)\n",
        "\n",
        "print(\"\\nNew Distribution across categories:\")\n",
        "print(balanced_papers_v2['primary_category'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLkx2F1Q6KHr",
        "outputId": "5ecb4dca-846b-40c7-ac83-e175f573a90b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching papers for cs.AI...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers: 100%|██████████| 80/80 [00:05<00:00, 14.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching papers for cs.LG...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers: 100%|██████████| 80/80 [00:05<00:00, 13.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching papers for cs.CL...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers: 100%|██████████| 80/80 [00:06<00:00, 13.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching papers for stat.ML...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers: 100%|██████████| 80/80 [00:13<00:00,  5.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fetching papers for cs.CV...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers: 100%|██████████| 80/80 [00:03<00:00, 26.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New Distribution across categories:\n",
            "primary_category\n",
            "cs.LG      40\n",
            "cs.CV      40\n",
            "cs.CL      40\n",
            "stat.ML    34\n",
            "cs.AI       5\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZeiQnNz7Zl_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}