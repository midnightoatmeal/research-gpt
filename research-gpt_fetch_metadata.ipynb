{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b74cdd-31be-4192-b10c-ee8d33c37c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d05e29c8-637b-4432-b747-72d077716c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (4.67.1)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
      "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=672ca3fa144e9aa1dba6851d69a36f5d00ca6df15917aee9a73a1ae90f0f4370\n",
      "  Stored in directory: /Users/midnight_oatmeal/Library/Caches/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8b969e-629d-4a62-9c6b-4348337697ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Attempt 1: Fetching available arXiv categories...\n",
      "‚úÖ SetSpec: cs - Computer Science\n",
      "‚úÖ SetSpec: econ - Economics\n",
      "‚úÖ SetSpec: eess - Electrical Engineering and Systems Science\n",
      "‚úÖ SetSpec: math - Mathematics\n",
      "‚úÖ SetSpec: physics - Physics\n",
      "‚úÖ SetSpec: physics:astro-ph - Astrophysics\n",
      "‚úÖ SetSpec: physics:cond-mat - Condensed Matter\n",
      "‚úÖ SetSpec: physics:gr-qc - General Relativity and Quantum Cosmology\n",
      "‚úÖ SetSpec: physics:hep-ex - High Energy Physics - Experiment\n",
      "‚úÖ SetSpec: physics:hep-lat - High Energy Physics - Lattice\n",
      "‚úÖ SetSpec: physics:hep-ph - High Energy Physics - Phenomenology\n",
      "‚úÖ SetSpec: physics:hep-th - High Energy Physics - Theory\n",
      "‚úÖ SetSpec: physics:math-ph - Mathematical Physics\n",
      "‚úÖ SetSpec: physics:nlin - Nonlinear Sciences\n",
      "‚úÖ SetSpec: physics:nucl-ex - Nuclear Experiment\n",
      "‚úÖ SetSpec: physics:nucl-th - Nuclear Theory\n",
      "‚úÖ SetSpec: physics:physics - Physics (Other)\n",
      "‚úÖ SetSpec: physics:quant-ph - Quantum Physics\n",
      "‚úÖ SetSpec: q-bio - Quantitative Biology\n",
      "‚úÖ SetSpec: q-fin - Quantitative Finance\n",
      "‚úÖ SetSpec: stat - Statistics\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import time\n",
    "\n",
    "def fetch_arxiv_sets():\n",
    "    \"\"\"Fetches and prints available OAI-PMH categories from arXiv.\"\"\"\n",
    "    base_url = \"http://export.arxiv.org/oai2\"\n",
    "    params = {\"verb\": \"ListSets\"}\n",
    "\n",
    "    for attempt in range(3):  # Retry mechanism\n",
    "        try:\n",
    "            print(f\"üì° Attempt {attempt + 1}: Fetching available arXiv categories...\")\n",
    "            response = requests.get(base_url, params=params, timeout=10)  # 10s timeout\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "            print(f\"‚ö†Ô∏è Warning: Status {response.status_code}. Retrying...\")\n",
    "            time.sleep(2)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Connection error: {e}. Retrying...\")\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        print(\"‚ùå Failed after multiple attempts.\")\n",
    "        return\n",
    "    \n",
    "    data = xmltodict.parse(response.content)\n",
    "    list_sets = data.get(\"OAI-PMH\", {}).get(\"ListSets\", {}).get(\"set\", [])\n",
    "\n",
    "    if isinstance(list_sets, dict):  # Single entry case\n",
    "        list_sets = [list_sets]\n",
    "\n",
    "    if list_sets:\n",
    "        for s in list_sets:\n",
    "            set_spec = s.get(\"setSpec\", \"N/A\")\n",
    "            set_name = s.get(\"setName\", \"Unknown Name\")\n",
    "            print(f\"‚úÖ SetSpec: {set_spec} - {set_name}\")\n",
    "    else:\n",
    "        print(\"‚ùå No category sets found.\")\n",
    "\n",
    "fetch_arxiv_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93bf202-ed0e-4897-8b6b-cdf3bbd85c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [01:22<00:00, 24.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset saved as arxiv_papers.json and arxiv_papers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_arxiv_papers(categories, max_results=2000, batch_size=200, delay=1):\n",
    "    client = arxiv.Client()\n",
    "    papers = []\n",
    "    \n",
    "    for cat in categories:\n",
    "        search = arxiv.Search(query=f\"cat:{cat}\", max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
    "        batch_count = 0\n",
    "        \n",
    "        for result in tqdm(client.results(search), total=max_results):\n",
    "            papers.append({\n",
    "                \"title\": result.title,\n",
    "                \"abstract\": result.summary,\n",
    "                \"published\": result.published.strftime('%Y-%m-%d'),\n",
    "                \"url\": result.entry_id,\n",
    "                \"authors\": [author.name for author in result.authors],\n",
    "                \"category\": result.primary_category\n",
    "            })\n",
    "            batch_count += 1\n",
    "            \n",
    "            if batch_count % batch_size == 0:\n",
    "                time.sleep(delay)  # Respect arXiv rate limits\n",
    "    \n",
    "    return papers\n",
    "\n",
    "# Define categories to scrape (Deep Learning focused)\n",
    "categories = [\"cs.LG\", \"cs.CL\", \"stat.ML\"]\n",
    "\n",
    "# Fetch papers with batch processing and delay\n",
    "dataset = fetch_arxiv_papers(categories, max_results=2000, batch_size=200, delay=1)\n",
    "\n",
    "# Save dataset as JSON\n",
    "with open(\"arxiv_papers.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=4)\n",
    "\n",
    "# Save dataset as CSV\n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_csv(\"arxiv_papers.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Dataset saved as arxiv_papers.json and arxiv_papers.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eeb4c67-8afd-4ecf-bc91-7543c93eccdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0   Low-Rank Adapting Models for Sparse Autoencoders   \n",
      "1  Redefining Machine Unlearning: A Conformal Pre...   \n",
      "2  Detection Is All You Need: A Feasible Optimal ...   \n",
      "3  Vintix: Action Model via In-Context Reinforcem...   \n",
      "4         Scalable-Softmax Is Superior for Attention   \n",
      "\n",
      "                                            abstract   published  \\\n",
      "0  Sparse autoencoders (SAEs) decompose language ...  2025-01-31   \n",
      "1  Machine unlearning seeks to systematically rem...  2025-01-31   \n",
      "2  We study the problem of piecewise stationary b...  2025-01-31   \n",
      "3  In-Context Reinforcement Learning (ICRL) repre...  2025-01-31   \n",
      "4  The maximum element of the vector output by th...  2025-01-31   \n",
      "\n",
      "                                 url  \\\n",
      "0  http://arxiv.org/abs/2501.19406v1   \n",
      "1  http://arxiv.org/abs/2501.19403v1   \n",
      "2  http://arxiv.org/abs/2501.19401v1   \n",
      "3  http://arxiv.org/abs/2501.19400v1   \n",
      "4  http://arxiv.org/abs/2501.19399v1   \n",
      "\n",
      "                                             authors category  \n",
      "0   ['Matthew Chen', 'Joshua Engels', 'Max Tegmark']    cs.LG  \n",
      "1                        ['Yingdan Shi', 'Ren Wang']    cs.LG  \n",
      "2  ['Argyrios Gerogiannis', 'Yu-Han Huang', 'Subh...    cs.LG  \n",
      "3  ['Andrey Polubarov', 'Nikita Lyubaykin', 'Alex...    cs.LG  \n",
      "4                               ['Ken M. Nakanishi']    cs.CL  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      6000 non-null   object\n",
      " 1   abstract   6000 non-null   object\n",
      " 2   published  6000 non-null   object\n",
      " 3   url        6000 non-null   object\n",
      " 4   authors    6000 non-null   object\n",
      " 5   category   6000 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 281.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"arxiv_papers.csv\")\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3812419f-a37c-45da-8d30-d5d4d650dc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved as cleaned_arxiv_papers.csv\n"
     ]
    }
   ],
   "source": [
    "# remove the noise and make clean data\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text) # remove citations like [1], [2]\n",
    "    text = re.sub(r\"\\(.*?\\)\", \"\", text) #remove anything in brackets\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() # remove extra spaces\n",
    "\n",
    "df[\"cleaned_abstract\"] = df[\"abstract\"].apply(clean_text)\n",
    "\n",
    "#drop any rows where abstract is missing\n",
    "df = df.dropna(subset=[\"cleaned_abstract\"])\n",
    "\n",
    "df.to_csv(\"cleaned_arxiv_papers.csv\", index=False)\n",
    "print(\"Cleaned dataset saved as cleaned_arxiv_papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71dfc949-c38c-4eb2-9c42-6957fa0c6c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fined-tuning dataset saved as fine_tune_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# converting the dataset\n",
    "\n",
    "import json\n",
    "train_data = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    train_data.append({\n",
    "        \"instruction\": \"Summarize this research paper.\",\n",
    "        \"input\": row[\"cleaned_abstract\"],\n",
    "        \"output\": \"This papers explores advances in deep learning.\"\n",
    "    })\n",
    "# save as JSONL\n",
    "with open(\"fine_tune_data.jsonl\", \"w\") as f:\n",
    "    for entry in train_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "print(\"Fined-tuning dataset saved as fine_tune_data.jsonl\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17cd1fa9-c780-4730-ab91-02f20149a25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fine_tune_data.jsonl', 'research-gpt_fetch_metadata.ipynb', 'arxiv_papers.csv', 'arxiv_papers.json', '.ipynb_checkpoints', 'cleaned_arxiv_papers.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c47a4-f477-4389-bbef-9276329cbbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
